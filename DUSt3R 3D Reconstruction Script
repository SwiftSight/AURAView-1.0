from dust3r.inference import inference
from dust3r.model import AsymmetricCroCo3DStereo
from dust3r.utils.image import load_images
from dust3r.image_pairs import make_pairs
from dust3r.cloud_opt import global_aligner, GlobalAlignerMode
from dust3r.utils.geometry import find_reciprocal_matches, xy_grid
import numpy as np
from matplotlib import pyplot as plt
import torch
from pathlib import Path
import logging
from tqdm import tqdm
import plotly.graph_objects as go
from typing import List, Tuple, Dict, Optional
import cv2

class DUSt3RProcessor:
    def __init__(
        self,
        model_name: str = "naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt",
        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
        batch_size: int = 2,
        schedule: str = 'cosine',
        lr: float = 0.01,
        niter: int = 300,
        image_size: int = 512
    ):
        self.device = device
        self.batch_size = batch_size
        self.schedule = schedule
        self.lr = lr
        self.niter = niter
        self.image_size = image_size
        
        # Setup logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Initialize model
        self.logger.info(f"Loading model {model_name} on {device}")
        self.model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)
        self.model.eval()

    def process_images(
        self,
        image_paths: List[str],
        scene_graph: str = 'complete',
        prefilter: Optional[str] = None
    ) -> Dict:
        """Process images and return reconstruction results"""
        # Load and preprocess images
        self.logger.info("Loading and preprocessing images...")
        images = load_images(image_paths, size=self.image_size)
        
        # Enhanced image preprocessing
        processed_images = []
        for img in images:
            # Apply CLAHE for better contrast
            if len(img.shape) == 3:
                lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
                lab[...,0] = clahe.apply(lab[...,0])
                img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)
            processed_images.append(img)

        # Create image pairs
        pairs = make_pairs(
            processed_images,
            scene_graph=scene_graph,
            prefilter=prefilter,
            symmetrize=True
        )

        # Run inference
        self.logger.info("Running inference...")
        with torch.no_grad():
            output = inference(pairs, self.model, self.device, batch_size=self.batch_size)

        # Global alignment
        self.logger.info("Computing global alignment...")
        scene = global_aligner(
            output,
            device=self.device,
            mode=GlobalAlignerMode.PointCloudOptimizer
        )
        
        # Optimize with progress bar
        losses = []
        pbar = tqdm(total=self.niter, desc="Optimizing alignment")
        def callback(loss):
            losses.append(loss)
            pbar.update(1)
            
        scene.compute_global_alignment(
            init="mst",
            niter=self.niter,
            schedule=self.schedule,
            lr=self.lr,
            callback=callback
        )
        pbar.close()

        return {
            'scene': scene,
            'output': output,
            'images': processed_images,
            'losses': losses
        }

    def visualize_reconstruction(
        self,
        results: Dict,
        save_path: Optional[str] = None,
        interactive: bool = True
    ):
        """Enhanced visualization of the reconstruction"""
        scene = results['scene']
        
        # Get reconstruction data
        imgs = scene.imgs
        focals = scene.get_focals()
        poses = scene.get_im_poses()
        pts3d = scene.get_pts3d()
        confidence_masks = scene.get_masks()

        # Create interactive 3D visualization
        if interactive:
            points = []
            colors = []
            for i, (pts, img, mask) in enumerate(zip(pts3d, imgs, confidence_masks)):
                valid_pts = pts[mask.cpu().numpy()]
                valid_colors = img[mask.cpu().numpy()]
                points.append(valid_pts.detach().cpu().numpy())
                colors.append(valid_colors)

            all_points = np.concatenate(points, axis=0)
            all_colors = np.concatenate(colors, axis=0)

            fig = go.Figure(data=[
                go.Scatter3d(
                    x=all_points[:, 0],
                    y=all_points[:, 1],
                    z=all_points[:, 2],
                    mode='markers',
                    marker=dict(
                        size=2,
                        color=[f'rgb({r},{g},{b})' for r, g, b in (all_colors * 255).astype(int)],
                        opacity=0.8
                    )
                )
            ])

            # Add camera positions
            camera_positions = poses.detach().cpu().numpy()
            fig.add_trace(go.Scatter3d(
                x=camera_positions[:, 0, 3],
                y=camera_positions[:, 1, 3],
                z=camera_positions[:, 2, 3],
                mode='markers+text',
                marker=dict(size=8, color='red'),
                text=[f'Camera {i}' for i in range(len(camera_positions))],
                name='Cameras'
            ))

            fig.update_layout(
                scene=dict(
                    xaxis_title='X',
                    yaxis_title='Y',
                    zaxis_title='Z',
                    aspectmode='data'
                ),
                title='3D Reconstruction'
            )

            if save_path:
                fig.write_html(f"{save_path}_3d.html")
            fig.show()

        # Visualize matches between image pairs
        self.visualize_matches(results, save_path)
        
        # Plot optimization losses
        if results.get('losses'):
            plt.figure(figsize=(10, 5))
            plt.plot(results['losses'])
            plt.title('Optimization Loss')
            plt.xlabel('Iteration')
            plt.ylabel('Loss')
            if save_path:
                plt.savefig(f"{save_path}_loss.png")
            plt.show()

    def visualize_matches(
        self,
        results: Dict,
        save_path: Optional[str] = None,
        n_viz: int = 20
    ):
        """Visualize matches between image pairs with enhanced graphics"""
        scene = results['scene']
        imgs = scene.imgs
        pts3d = scene.get_pts3d()
        confidence_masks = scene.get_masks()

        pts2d_list, pts3d_list = [], []
        for i in range(len(imgs)):
            conf_i = confidence_masks[i].cpu().numpy()
            pts2d_list.append(xy_grid(*imgs[i].shape[:2][::-1])[conf_i])
            pts3d_list.append(pts3d[i].detach().cpu().numpy()[conf_i])

        for i in range(len(imgs)-1):
            reciprocal_in_P2, nn2_in_P1, num_matches = find_reciprocal_matches(
                pts3d_list[i],
                pts3d_list[i+1]
            )
            
            matches_im1 = pts2d_list[i+1][reciprocal_in_P2]
            matches_im0 = pts2d_list[i][nn2_in_P1][reciprocal_in_P2]

            # Enhanced visualization
            match_idx_to_viz = np.round(
                np.linspace(0, num_matches-1, min(n_viz, num_matches))
            ).astype(int)
            
            viz_matches_im0 = matches_im0[match_idx_to_viz]
            viz_matches_im1 = matches_im1[match_idx_to_viz]

            H0, W0 = imgs[i].shape[:2]
            H1, W1 = imgs[i+1].shape[:2]
            
            # Create concatenated image
            img0 = np.pad(imgs[i], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), 'constant')
            img1 = np.pad(imgs[i+1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), 'constant')
            img = np.concatenate((img0, img1), axis=1)

            plt.figure(figsize=(20, 10))
            plt.imshow(img)
            
            # Use a rainbow colormap for better visibility
            cmap = plt.get_cmap('rainbow')
            for j in range(len(viz_matches_im0)):
                (x0, y0), (x1, y1) = viz_matches_im0[j], viz_matches_im1[j]
                color = cmap(j / (len(viz_matches_im0) - 1))
                plt.plot(
                    [x0[0], x1[0] + W0],
                    [x0[1], y1[1]],
                    '-o',
                    color=color,
                    linewidth=1.5,
                    markersize=5,
                    alpha=0.7
                )

            plt.title(f'Matches between images {i} and {i+1} (showing {len(viz_matches_im0)} out of {num_matches} matches)')
            plt.axis('off')
            
            if save_path:
                plt.savefig(f"{save_path}_matches_{i}_{i+1}.png", bbox_inches='tight')
            plt.show()

def main():
    # Example usage
    processor = DUSt3RProcessor(
        batch_size=2,
        lr=0.015,
        niter=400,
        image_size=512
    )
    
    # Process images
    results = processor.process_images(
        ['croco/assets/Chateau1.png', 'croco/assets/Chateau2.png'],
        scene_graph='complete'
    )
    
    # Visualize results
    processor.visualize_reconstruction(
        results,
        save_path='reconstruction_output',
        interactive=True
    )

if __name__ == '__main__':
    main()
